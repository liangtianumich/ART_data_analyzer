#!/usr/bin/env python
import os, json, pickle
import numpy as np
import time
import random
import multiprocessing as mp
import argparse
from event_selector import filter_events_all_tests_stage_1,filter_events_all_tests_stage_2,find_act_relax_equal_events
from calculator.event_energy_calculator import *
from calculator.strain_calculator import strain_calculator_run_all_tests_mp
from calculator.voronoi_structural_analysis import run_all_tests_voronoi_calculator, run_all_tests_voronoi_classifier
from calculator.participation_number_calculator import pn_calculator_run_all_tests_mp
from visualizer.strain_visualizer import events_strain_visualization,strain_events_stats_visualization
from correlation_model.correlation_model import residual_threshold_finder, all_events_local_atoms_finder, events_local_atoms, eng_max_disp, shear_strain_vol_strain_cluster_all_events,shear_strain_vol_strain_local_atom_all_events
from correlation_model.generate_correlation_table import generate_correlation_table_mp
from data_reader import *
from util import run_tests_triggered_atom_is_max_disp,nostdout,prompt_yes_no,all_events_triggered_cluster_atoms_finder,all_events_central_atom_finder, all_events_max_disp_atom_finder, sync_central_atom_list_w_curr_subdir
#from lammps.run import run_lammps
from ART_wrapper.run import run_art_mp, set_up_input_files, delete_art_tests_files, delete_art_tests, check_tests_status, delete_unused_events_data
from ART_wrapper.run_slurm import run_art_cluster_slurm
from archive import archive_project
from file_converter import refconfig_to_lammps_data

def print_input_help():
	"""
	this function print the help to set up input file in a json format
	"""
	print \
	"""
	art_data --settings / -s need an input file in json format specifies the
	following keys (keys are organized based on how they are used):
	1) ART_running needed keys:
	
	  path_to_data_dir: str,
		path to the art data directory, get by os.environ['DATA_DIR'] in environment.sh
	  
	  path_to_input_files: str,
		path to ART input files, get by os.environ['ART_INPUT'], default
		the same as os.environ['DATA_DIR'] in environment.sh
	
	  total_energy: float,
		total potential energy of the ART sample
		currently, can be any float number, e.g. 0.0, ART will re_calculate total energy during initial minimization
	  
	  sample_name: str,
		file name of the sample configuration file containing atomic coordinates
		get by os.environ['ART_SAMPLE'] in environment.sh, e.g conf.lammps
	  
	  sample_type: str,
		sample_type can be either 'dump' or 'lammps_data', get by
		os.environ['SAMPLE_TYPE'] in environment.sh
	  
	  box_range: list,
		the simulation box range in x,y,z direction
	    e.g. [[-27,27], [-27,27], [-27,27]], default read the box
		range from the sample configuration file, i.e sample_name

	  box_dim: list,
		the orthogonal simulation box dimension along x, y, z, default read the box
		dimension from the sample configuration file, i.e sample_name
	
	  central_atom_list: list
		a list of central atoms id used to create all test directories,
		default, created based on randomly select user specified number of atoms
		out of total interested atoms (Here total interested atoms can be a subset of all atoms in whole sample). 
		A list of selected atoms' atom id will be central_atom_list
	  
	2) In addition to the above keys, ART post-processing needed keys:
	  
	  list_of_test_id: list
		list of the tests to be post-processed, default all tests calculated, the same as central_atom_list
	  
	  identical_event_criteria: dict
		stage 2 criteria to filter redudantant events, user need to customize 
		the values of three keys "D_init_fin", "E_init_fin", "E_init_sad"
		these value can be different for different type of samples. 
		A good starting point for metallic materials, e.g. 
		{"D_init_fin": 0.1, "E_init_fin": 0.005, "E_init_sad": 0.01}
		this means that event pair is redundant if
		abs(D(fin - init)_1-D(fin-init)_2) < 0.1 (A)
		AND abs(E(fin-init)_1-E(fin-init))_2 < 0.005 (eV)
		AND abs(E(sad-init)_1-E(sad-init))_2 < 0.01 (eV)
	  
	  significance:
		the significance level of we can not reject that mean are equal
		during two sample student t test
		Significance Level: alpha
		Critical Region: Reject the hypothesis that the two means are equal if
		|T|> t1-alpha/2,v
		where t1-alpha/2,v is the critical value of the t distribution with v degrees of freedom
	  
	  re_calc: boolean
	    default False
		decide whether to override prior existing calculations, True or False
		if the user only want to re-do calculation for a single calculation,
		then he can either
		1) modify the re_calc to be True in input SETTINGS file, then 
		use this input file for once and then change it back to be False
		or
		2) add --re_calc argument when using art_data command line tool
	  
	  cut_off: dict
		cut_off distance for different atom type pair when determining the nearest neighbor
		eg. {str((1,1)):3.7,str((1,2)):3.2,str((2,2)):3.5}
		means atom type 1 and type 1 cut off distance is 3.7 A
		atom type 1 and type 2 cut off distance is 3.2 A
		atom type 2 and type 2 cut off distance is 3.5 A
		The cut-off values for different atom type pairs can be obtained from
		Ovito by selected particle type, delete selected particles, coordination analysis.
		
	  num_of_proc: integer
		number of processors used to run a task
	    default, max number of cores, mp.cpu_count()		
	  
	  atom_list: None or 'all', 'local', 'initial','central','max_disp' or a list
	    default None
		if None or 'all', calculate all atoms in the configuration
		if 'local', calculate the local atoms, there must be a file called local_atoms_index.json
		existing for init_sad, sad_fin, init_fin of each event, this local_atoms_index.json is generated automatically
		by the machine learning outlier detection of local atoms.
		if 'initial', calculate the initial triggered cluster atoms saved in the file called initial_cluster_atoms_index.json
		for each filtered event in list_of_test_id
		if 'central', calculate the initial central atom saved in the file called central_atom_index.json
		for each filtered event in list_of_test_id
		if 'max_disp', calculate the max displacement atom saved in the file called max_disp_atom_index.json
		for each filtered event in list_of_test_id
		if type of atom_list is list, then the list stores the item id of atoms
	
	  # keys below are used for the machine learning outlier detection of local atoms
	  
	  model: sklearn machine learning model used to do outlier detection
		default "LinearSVR", option: "linear_model", "linear_RANSAC" and "LinearSVR"
		"linear_model" is regular least square linear regression that maximize the probability of observing the training data
		"linear_RANSAC" and "LinearSVR" see scikit-learn description.
		
	  
	  feature: features used as input
		default "displacement", feature used for outlier detection of local atoms
	  
	  target: targeted properties as model output
		default "shear_strain", target used for outlier detection of local atoms
	  
	  residual_threshold: list of relative residual threshold candidates 
	    to determine the best residual threshold
	    default, np.arange(0.01, 1.0, 0.01).tolist()
	  
	  final_residual_threshold: float,
	    relative residual threshold used to determine the local atoms index
	    default, 0.54.  This value will be obtained after doing the
	    relative residual threshold parameter sweep. 
	  
	  critical_local_atoms_slope: float
	    the critical slope in the double slope stopping convergence criteria 
	    when plotting local atoms vs relative residual threshold. 
	    currently, default -15.  User need to customize this value for their system
	
	  # keys below are for voronoi analysis
	  
	  voro_cut_off: float, 
	    cut_off to determine the nearest neighbors in voronoi analysis
	    a single float value, should be max of the cut-off for different atom types, e.g.3.7
	    ensure that this cut_off value will contain all atoms that could possibly form the voronoi
	    polyhedron, check the ovito radial distribution function of current sample
	    to get a feeling of how much a proper value would be. 
	  
	  periodic: list of 3 Boolean elements
		specify the perioidic condition in x,y,z direction
	    for 3D periodic boundary condition, using [True, True, True]
	"""

def print_desc_help():
	"""
	this function print the description about how to use art_data command line
	workflow
	"""
	print \
	"""
	This art_data command line tool is expected to perform the following tasks:
	
	1) automating the process of running ART simulations in parallel 
	to generate ART data (tens of GBs) of the molecular dynamics created configuration sample
	
	2) automating various post-processing tasks of ART data in a user workflow in parallel.
	including but not limited to: save all most necessary calculations in files, 
	, extract data for automated pretty plotting, perform correlation analysis between
	physical quantities.
	
	Cite this software: 
	Liang Tian, Lin Li, Jun Ding, Normand Mousseau, 
	"ART_data_analyzer: automating parallelized computations to study the evolution of materials"
	Software X, (in press)
	
	>>> automate running ART in parallel:
	This package has been written in a general way to be robust to a wide range of
	homogeneous and heterogenous samples, e.g. crystal, interface, grain boundary,amorphous materials. 
	
	The default setting is that the user are interested in the energy barrier distribution from the whole MD sample.
	
	If user are only interested in the energy barrier distribution from a subsection of the MD sample,
	(e.g. grain boundary atoms out of a bicrystal sample), user need to create a file
	called interested_atom_list.json, which contains a list of all interested sub-section
	atom_ids under the directory path_to_data_dir. path_to_data_dir has been set up by the user in 
	environment.sh as DATA_DIR.
	If this interested_atom_list.json file does not exist, central atoms list will be the list of all atoms in whole sample
	unless we want to perturb less central atoms (as specified in --example n_tests)
	than all atoms in the whole sample.
	
	Notes about why we can not just use the configuration of the subsection:
	the whole MD configuration sample are still used by ART as the configuration sample file to ensure
	that the initial minimized MD sample configuration does not deviates from the ART 
	initial minimized configuration too much so that we are truly probing the Potential Energy Landscape(PEL) 
	of the initial MD sample.
	
	
	>>> Steps to set up running ART in parallel:
	
	Before this, you need to ensure that artn (built with lammps library) is installed correctly
	(including loading all necessary lammps package before building lammps),
	and PATH and LD_LIBRARY_PATH has been set up in your current terminal session (
	the same as when you are building lammps and artn)
	in order for artn and lammps to find the openmpi, openblas library.
	This has been documented in great details in the guide to install ART with lammps,
	which is located in /artn/ART_data_analyzer/readme.txt
	
	Please read this ART installation guide carefully since it contains various common reasons
	user can encounter. 
	
	
	1) modify environment.sh:
	modify DATA_DIR to your desired directory path to save the ART data,
	Generally, we do not need to modify other environmental variables,
	can be the same as default. Current default is that ART_INPUT storing path_to_input_files 
	are set up to be the same as path_to_data_dir so that all ART input files
	should be located under path_to_data_dir.
	
	The default sample is lammps_data type of sample, whose name must be
	modified to conf.lammps. This lammps data conf.lammps file can be
	converted from dump file by Ovito easily.
	export ART_SAMPLE=conf.lammps
	export SAMPLE_TYPE=lammps_data
	
	Sometimes, if user need to use the dump file sample, they can modify
	the ART_SAMPLE to the dump file name (e.g.conf.dump),  SAMPLE_TYPE=dump
	export ART_SAMPLE=conf.dump
	export SAMPLE_TYPE=dump
	
	if necessary modify MY_ART. If you can see this print-out, it also means
	that you are using art_data command line tool, ART_data_analyzer is in your path.
	No need to modify
	
	2) source environment.sh
	
	3) put 4 art input files in path_to_data_dir you just specified in $DATA_DIR
	
	They are bart.sh, conf.lammps, in.lammps, potential file. refconfig file will
	be automatically created based on lammps_data file conf.lammps or lammps dump file
	conf.dump.  When sample_type is dump, user need to pay attention if the atomic coordinates
	in lammps dump file is fractional or absolute, now the default is using fractional coordinates.
	Then user need to convert it to a dump file with fractional coordinates.
	
	> bart.sh
	Users need to be really careful to modify bart.sh to have the correct parameters,
	it is strongly suggested that user can refer to developers for further questions
	about a particular parameter if the artn user guide does not provide sufficient information.
	because bart.sh need tcsh shell instead of bash shell to run,
	set up environmental variables by setenv (case-sensitive) instead of export.
	
	Central_Atom in bart.sh do not need to be modified since art_data will automatically load
	each central atom in central_atom_list key in input SETTINGS file as Central_Atom
	
	a few important parameters to converge to the true saddle point are:
	Exit_Force_Threshold should be less than 0.1
	Incremental_Size 0.1, the atom move step size during artn running the saddle state convergence algorithm
	
	If the event is a local event, setenv LOCAL_FORCE .true. will invoke
	local force computation that will improve the computation speed substantially, from tens of times to 100 times.
	INNER_REGION_RADIUS need to be tested so that the elastically frozon inner regions contains about 800-1000 atoms,
	this will ensure energy barrier has an accuracy of about 0.01 eV.
	OUTER_REGION_WIDTH is determined by the forcefield length. 
	The purpose is to make sure that all atoms in the inner region have the right force applied on them.
	
	> in.lammps
	in.lammps default read lammps_data file conf.lammps by read_data conf.lammps.
	If the sample is dump file conf.dump, in.lammps need to tricked and tested by lammps to
	do read_dump XXX. In addition, the potential file name and style need to be specified correctly.
	
	> conf.lammps
	conf.lammps is the lammps data file that can be converted from dump file by Ovito.
	
	> potential file
	potential file need to be present. 
	It is worthwhile to note that some potential need specific lammps packages when building lammps.
	Otherwise, in.lammps can not read this potential.
	
	4) run the --example command to generate input SETTINGS file
	As mentioned before, if you are interested in sub-section (grain boundary or interface)
	of the whole sample,
	you need to create a file called interested_atom_list.json under path_to_data_dir
	to specify the interested sub-section. If this file does not pre-exist, --example
	will automatically generate the file interested_atom_list.json based on
	all atoms in the whole sample. 
	
	e.g. art_data --example 2000 > input_sample_id.json
	
	After this command, it will update the ART_running needed keys 
	(these keys can be checked by art_data --settings-format) in the
	input_sample_id.json file. 
	
	The usage of this input SETTINGS file is
	'art_data -s SETTINGS' (where SETTINGS is the input file name or full path to input file name)
	For the example above, SETTINGS is input_sample_id.json
	
	User no longer need to modify these ART_running needed keys manually any more.
	
	In the command above, 
	2000 is the number of tests user want to calculate. Each test directory
	name will be the central atom id. Each test directory may contain multiple events
	specified by the Max_Number_Events in bart.sh ART input parameters file.
	
	If the number of atoms in interested_atom_list.json is less than 2000, 
	it will just run each of all atoms in interested_atom_list.json as central_atom for each test. 
	Otherwise, it will randomly select 2000 atoms from interested_atom_list.json as central_atom
	to calculate. For both cases, after this command, the actual list of central_atoms 
	to be calculated will be saved in a file called central_atom_list.json, which is created
	automatically.
	
	input_sample_id.json is an example input SETTINGS file name. sample_id
	is a placeholder that can be something related to the sample,
	e.g the sample is created under cooling rate 10E10. Then input_10E10.json
	is a good name.
	
	5) run the command: art_data -s input_sample_id.json --art --run
	where input_sample_id.json is the same file you created in step 4. 
	After user type this command, it will prompt in command line to type
	the potential file name (located inside the path_to_input_files).
	If user type the wrong potential file name, it will not copy this potential file
	and running art will give error.
	
	It now will ran all tests under the path_to_data_dir in parallel. 
	The input SETTINGs file input_sample_id.json contains the key num_of_proc,
	which specify the number of cores to run in parallel. Default using all cores
	in the local machine. This command will automatically set up the input files 
	for running art by running ./mod_bart.sh inside each test dir.
	
	art_data -s input_sample_id.json --art --run now also support running in slurm cluster environment.
	However, user need to coordinate with the slurm admistrator to set up the necessary environment
	for running artn before running in slurm environment.
	This can be done by art_data -s input_sample_id.json --art --run --slurm 4
	will use 4 compute nodes
	Now the num_of_proc key in input_sample_id.json means the number of compute nodes
	user requested to submit their jobs to. This will split all central atoms in central_atom_list key
	into num_of_proc folds. Each fold will be allocated to one compute node 
	as a single submitted sbatch job to use its all available cores to 
	perform parallel computation using python multiprocesssing on this single compute node. 
	
	After finishing running art, it may be necessary to archive all necessary
	raw art data into a zip file for the purpose of sharing raw research data
	between researchers.  This can be done easily through --archive command,
	e.g. art_data -s SETTINGS --archive
	will archive 
	1) all regular files in path_to_data_dir, such as final_selected_events.json, central_atom_list.json, interested_atom_list.json
	2) all necessary configuration files related to the final
	filtered events saved in final_selected_events.json
	3) log.file.1, events.list, bart.sh or mod_bart.sh in final events' test dir
	
	Note 1: --archive command will also automatically update central_atom_list.json file to be
	the test ids that are saved in final_selected_events.json. In the command line,
	user need to confirm y/n on whether to update central_atom_list.json file.
	For any purpose, if user need to save original central_atom_list.json file, they
	can do if before this confirmation
	
	>>> 
	If user are not sure whether central_atom_list.json has already been updated after un-zipping
	the art_data_project.zip file, they can always update this file to be the newest by using
	
	>>> art_data -s input_settings.json --art --delete_unused_events
	
	This command delete nothing in the un-zipped project, but it will update
	central_atom_list.json file.  After this file is updated, user then 
	need to update the central_atom_list and list_of_test_id key in input settings file by
	
	>>> art_data -s input_settings.json --update_input
	
	After these steps, we get all most necessary final events data in the most compacted size, along with all
	project status files (central_atom_list.json, input_settings.json) ready for the following post-processing tasks
	
	Note 2: if running art in parallel quit unexpectedly due to either machine failure or human error,
	it is good practice to check the status of all tests by using
	art_data -s input_sample_id.json --art --check_tests_status
	
	This will create two input SETTINGS files called input_tests_done.json 
	storing the id of finished tests and input_tests_undone.json storing 
	the id of unfinished tests
	
	For finished tests:
	user can check the results of only finished tests as described in the
	following post-processing section by e.g.
	art_data -s input_tests_done.json --filter, art_data -s input_tests_done.json --eng --calc
	
	For unfinished tests:
	user can choose to delete these unfinished tests completely by art_data -s input_tests_undone.json --art --delete_tests
	Then user can continue to run these unfinished tests from the beginning by art_data -s input_tests_undone.json --art --run
	
	
	>>> Steps to set up running ART post-processing in parallel:
	The previously created input SETTNGs file is needed for post-processing 
	of ART_data to run the following tasks.
	
	Once again, the same as mentioned above, Example 'art_data' JSON input SETTINGS files can be output by the
	  'art_data --example' options:
		e.g. art_data --example n_tests > input.json
		where n_test specifies the total number of tests for each central atom ids
	
	However, the user should manually edit the parameters related to ART post-processing 
	in this input settings file as needed for the following tasks to proceed.
	
	Notes about some parameters in input SETTINGs file:
	
	list_of_test_id: a list of interested test ids that will be post-processed
	If the test whose test_id is 1, art_data will search 
	the test directory naming either test1 or 1 in path_to_data_dir
	Command line argument override rule:
	num_of_proc: The number of processors/cores used will be specified preferably
	in the input file, if this information does not exists in input file, 
	it will use the value by the command line argument -np;
	path_to_data_dir: will be specified preferably by the command line argument -p;
	if this information does not exist, it will use the path_to_data_dir in input file.
	re_calc: will be specified preferably by the command line argument --re_calc
	if --re_calc does not exist, it will use the re_calc key value in input file.

	To check the meaning of each key in input file, see 'art_data --settings-format' for help.
	
	
	>>>Post-processing tasks in a user workflow:
	
	1) filter events:
	
	  Before this operation, the user should have at least one SET of ART data
	  available (after finishing ART running)
	  checking the rough convergence of filtered events and
	  checking the convergence of activation energy and relaxation energy by ttest.
	  
	  'art_data -s SETTINGS --filter'
	  will filter all events in tests stored in list_of_test_id under path_to_data_dir 
	  (-p argument will override the input file SETTINGS)
	  it will invoke function event_selector.filter_events_all_tests_stage_1
	  and filter_events_all_tests_stage_2 and filter events based on 
	  three criteria implemented.
	  
	  This step can take a few hours to complete due to expensive event pair comparison in
	  stage 2. 
	  
	  Currently, stage 1 filtering two criteria are based on a single event,
	  implemented as constant criterias
	  For a single event, if the saddle state energy is less than any of initial or final state,
	  this event is not successful. 
	  E(sad-init) <0 or E(sad - fin) <0,
	  
	  For a single event, if both the energy difference AND distance between the final state and initial state 
	  are small, then the final state is identical to the initial state,
	  this event is not successful. The criteria value are chosen as:
	  dE < 0.02 (eV) AND distance < 1
	  
	  stage 2 filtering criteria are for comparing event pairs to remove redundant events.
	  There are 3 threshold values needed to be specified by the user in the
	  input SETTINGS file's identical_event_criteria key. For example,
	  "identical_event_criteria key": {"D_init_fin": 0.1, "E_init_fin": 0.005, "E_init_sad": 0.01}
	  User need to customize the threshold values for "D_init_fin", "E_init_fin",
	  "E_init_sad" for their studied system. 
	  
	  These filtering criteria was brought up by Prof Yue Fan at University of Michigan,
	  proved to be successful in his Paper "How thermally activated deformation starts in metallic glass".
	  
	  2) perform activation and relaxation energy convergence tests:
	  
	  'art_data -s SETTINGS --eng --calc' will invoke function
	  event_energy_calculator.energy_calculator_run_all_tests_mp to calculate
	  the activation energy and relaxation energy for all filtered events in
	  list_of_test_id under path_to_data_dir.
	  
	  If users want to get the activation and relaxation energy statistics from a subset of
	  all tests data (mainly because this subset of tests corresponds to triggered atoms within a specific category having a physical meaning),
	  they can copy the input SETTINGS file and modify the key "list_of_test_id" in the copied input SETTINGS file to be the list of the subset.
	  Then they can use this modified input file for --eng --calc for updated plots and print-out data statistics.
	  
	  User can choose to run either one SET or a few SET of ART data with different paramters to 
	  ensure post-filtering activation energy distribution indeed converge by a
	  statistical test. 
	  
	  Currently, the statistical tests can be either by checking the mean of activation energy
	  diff by less than 0.01 eV (since the artn local force evaluation on inner region atoms
	  from 800 to 1500 atoms has a barrier precision ~0.01 eV)
	  as we calculate more tests
	  or 
	  by implementing t test by --ttest argument.
	  --ttest should be much more rigorous convergence criteria. User may need to
	  calculate much more tests if using ttest convergence criteria
	  
	  --ttest argument will work on the energy data calculated above. ttest
	  need the significance key word value in input SETTINGS file, if not in
	  input SETTINGS file, use default value 0.05
	  
	  Reject the hypothesis that the two means are equal if
	  |T|> t1-alpha/2,v, where t1-alpha/2,v is the critical value of 
	  the t distribution with v degrees of freedom, significance Level alpha
	  
	  
	  The current ttest argument has two modes: 
	  
	  'art_data -s SETTINGS --eng --ttest OPTION PATH_1 PATH_2' will invoke function
	  event_energy_calculator.eng_convergence_ttest_ind if OPTION is ind 
	  or event_energy_calculator.eng_convergence_ttest_rel if OPTION is rel
	  , where PATH_1 is the path string to the ART data directory 1 and PATH_2
	  is the path string to the ART data directory 2.
	  
	  'art_data -s SETTINGS --eng --ttest_kfold OPTION k n' will
	  randomly divide the ART data in path_to_data_dir into k folds for n different times
	  and perform t test on each fold pair to ensure all fold pairs are convergent
	  
	  User need to test which ttest mode is a good convergence criteria, for example,
	  2 fold 3 times t tests
	  
	  >>>if ART data is not converged, then you can run more ART_data by the following command
	  
	  art_data -s SETTINGS --art --run_more N_TESTS
	  where N_TESTS is the number of tests to be calculated more.
	  Run more tests until you achieve the convergence.
	  This command will update the central_atom_list.json as well.
	  
	  If user happen to interrupt the command --run_more, the central_atom_list.json file
	  has already been updated. Then they need to use art_data -s SETTINGS --sync_central_atoms
	  to sync the central_atoms_list.json with current central atoms in path_to_data_dir before
	  using --run_more ever again. Then use --update_input to update the central_atom_list 
	  and list_of_test_id key in the input SETTINGS file
	  
	  Each time using --art --run_more command, it will also update the
	  input SETTINGS file 'central_atom_list' and 'list_of_test_id' key to be those
	  in central_atom_list.json.  If user want to manually update input SETTINGS, 
	  they can always do it by art_data -s SETTINGS --update_input
	  
	  After these --run_more commands, user need to repeat 
	  the event filtering (always use --filter --re_calc if user do --art --delete_unused_events after previous --run_more), 
	  energy calculation and checking the convergence again.
	  
	  If user only run more tests without changing filtering criteria, 
	  user need to use "art_data -s SETTINGS --filter" to filter new calculated tests.
	  It will generate accepted_events.json and selected_events.json in each new test directory 
	  and override final_selected_events.json in path_to_data_directory.
	  However, user need to add --re_calc argument to make keyword re_calc=True 
	  when re-do the event filtering with different filtering criteria
	  (Such as "art_data -s SETTINGS --filter --re_calc").
	  
	  User need to add --re_calc when re-do the energy calculation 
	  (Such as "art_data -s SETTINGS --eng --calc --re_calc").
	  This will override the "act_relax_eng_filtered_events.json" and 
	  "act_relax_eng_filtered_events.png" in path_to_data_dir
	  
	  and then re-do ttest by the same command above 
	  "art_data -s SETTINGS --eng --ttest_kfold OPTION k n". 
	  
	 Notes:
	 If user confirmed that their final filtered events statistics are converged,
	 all following calculations will be performed on the final filtered events,
	 there is no need to keep the unsuccessful events data to occupy disk space.
	 In this case, if user want to delete unused events data, they can do it by
	 >>> art_data -s input_settings.json --art --delete_unused_events
	 This command will first delete unused tests that are not saved in final_selected_events.json,
	 then update the central_atom_list.json after user's confirmation, then delete the unused events
	 configuration files
	 
	 3) run calculations and visualizations:
	 
	 "art_data -s SETTINGS --strain --calc" will invoke the strain and displacement calculations on specified atom_list
	 and automatically plots results for individual event and statistics
	 for all filtered events in tests stored in list_of_test_id, 
	 implemented in function calculator.strain_calculator.strain_calculator_run_all_tests_mp
	 
	 --strain --calc can be combined with any of --local, --central, --initial, --max_disp, --pn, --pn_non_affine
	 to select desired list of atoms to calculate strain and displacements. However,
	 usage of --local, --central, --initial, --max_disp, --pn, --pn_non_affine need to run --find_local_index,
	 --find_central_index, --find_triggered_cluster_atoms_index, --find_max_disp_index, --find_pn_index, --find_pn_non_affine_index first
	 
	 "art_data -s SETTINGS --strain -v" will plot the event level quantities after strain calculation on the selected atom_list
	 
	 "art_data -s input.json --strain --stats" will plot the statists of all events after strain calculation on the selected atom_list
	 
	 Note: 
	 In the above commands,
	 -p or --path option will overide the path_to_data_dir in input files
	 --re_calc will overide pre-existing calculations and plots
	 
	 atomic strain calculation is done by using the algorithm proposed
	 by Prof Ju Li at MIT:
	 http://li.mit.edu/A/Graphics/A/annotate_atomic_strain/Doc/main.pdf
	 
	 4) determine the local atom indexes of each filtered event in list_of_test_id
	 by a machine learning outlier detection algorithm
	 
	 This algorithm has been tested by Dr Liang Tian to work very well
	 on metallic glass sample using LinearSVR model between atomic displacement vs shear strain.
	 
	 art_data -s SETTINGS --find_residual will find
	 the critical relative residual threshold by invoking the function
	 correlation_model.correlation_model.residual_threshold_finder. 
	 
	 This algorithm need a parameter called relative residual threshold to
	 determine the critical relative residual threshold whether an atom should be called an outlier.
	 
	 The current criteria to determine the critical residual threshold is to 
	 do a relative residual threshold parameter sweep on their resultant 
	 average number of local atoms of all filtered events.
	 The critical relative residual threshold is the relative residual threshold 
	 when local atoms vs relative residual threshold curve reach convergence, 
	 where the convergence criteria is defined as double slope stopping criteria
	 to prevent slope fluctuation and be a more rigourous criteria.
	 The critical slope could vary for different types of samples.
	 
	 User can customize the critical slope by the key critical_local_atoms_slope
	 in the input SETTINGS file
	 
	 A reasonable value of this critical relative residual threshold, is usually 
	 around e.g. 0.54.
	 
	 -p or --path option applied, -np and --re_calc does not apply
	 
	 5) find all interested atoms indexes (e.g. local atoms, central atom, initial triggered cluster atoms, max_disp atom, pn atoms, pn_non_affine atoms) 
	 for each of filtered events
	 
	 art_data -s SETTINGS --find_local_index rel_residual_threshold,
	 will find the local atoms indexes for all filtered events in interested 
	 tests list_of_test_id, and save them in a file called local_atoms_index.json 
	 inside each event directory. This file save a dictionary with keys
	 "init_sad", "sad_fin", "init_fin" that contains the list of local atoms indexes 
	 
	 where rel_residual_threshold is the critical relative residual threshold found in step 4. 
	 
	 Function used: correlation_model.correlation_model.all_events_local_atoms_finder
	 
	 art_data -s SETTINGS --find_central_index will find the central atom index 
	 for each of all filtered events in interested tests list_of_test_id, 
	 and save them in a file called central_atom_index.json
	 
	 art_data -s SETTINGS --find_triggered_cluster_atoms_index will find the initial 
	 triggered cluster atoms indexes for each of all filtered events in interested tests list_of_test_id, 
	 and save them in a file called initial_cluster_atoms_index.json
	 
	 art_data -s SETTINGS --find_max_disp_index will find the max_disp atom index from
	 the initial to saddle process for each of all filtered events in interested tests list_of_test_id, 
	 and save them in a file called max_disp_atom_index.json
	 
	 art_data -s SETTINGS --find_pn_index will calculate the PN for init_sad, sad_fin, init_fin and save the
	 PN number of max displaced atoms' indexes into a file called pn_index.json, pn_number.json
	 The PN calculation can be checked from Swayamjyoti et al 2014, "Local structural excitations in model glasses"
	 
	 art_data -s SETTINGS --find_pn_non_affine_index will calculate the PN based on atomic shear strain and save the
	 PN number of max absolute shear strain atoms' indexes into a file called pn_index.json, pn_number.json
	 
	 -p/--path, -np, --re_calc option applied

	 6) if --strain --calc run on all atoms before,
	 now only run calculations for selected atoms (e.g, local atoms):
	 
	 Either manually edit the atom_list key in input SETTINGS file for once
	 or use the command line argument to overwrite
	 
	 > atomic strain and displacement:
	 
	 art_data -s SETTINGS --local --strain --calc will invoke the strain and displacement
	 calculations for local atoms only for all filtered events in interested tests stored in list_of_test_id.
	 If re_calc is False, it will just read the results file and extract the local atoms for strain and displacement statistics
	 If re_calc is True, it will re calculate the strains and displacements only on local atoms and overwritten the strain and displacement results file
	 of pre-existing calculations for all atoms strain and displacement
	 
	 It will rerun calculator.strain_calculator.strain_calculator_run_all_tests_mp
	 with atom_list = 'local',
	 which will read all local atom indexs saved in local_atoms_index.json
	 
	 Similar to --local, --central, --initial, --max_disp, --pn, --pn_non_affine also apply here
	 
	 7) correlation analysis
	 
	 Either manually edit the atom_list key in input SETTINGS file for once
	 or use the command line argument to overwrite
	 
	 After getting local_atoms_index.json, user can plot the local event cluster averaged
	 shear strain vs cluster averaged volumetric strain for all filtered events
	 in list_of_test_id by art_data -s SETTINGS --strain --local --cluster_shear_vol
	 or plot the shear strain vs volumetric strain for each local atom for
	 all filtered events by art_data -s SETTINGS --strain --local --atom_shear_vol
	 It will also fit a LinearRegression model on volumetric strain
	 to shear strain data, print out the slope as the beta parameter, i.e. 
	 the ratio of dilation to shear strain. 
	 Similar to --local, --central, --initial, --max_disp, --pn, --pn_non_affine also apply here for
	 other type of atom_list
	 
	 art_data -s SETTINGS --max_disp_eng will plot the activation energy vs max_disp
	 for all filtered events that are in list_of_test_id, if --subset exists,
	 also check if this event in subset_events.json
	 
	 > voronoi index calculation:
	 if voronoi_index_results.json exists in the event directory and re_calc is False,
	 it will just read this file
	 
	 If --return_volume is used and re_calc is False, both voronoi_index_results.json 
	 and voronoi_volume_results.json must exist in the event directory.
	 
	 If we want to re_calc for different atom_list (--central, --initial, --max_disp, --pn, --pn_non_affine),
	 use this arguments with --re_calc, e.g.
	 art_data -s SETTINGS --initial --voro --calc --re_calc will read
	 local_atoms_index.json and calculate local atoms voronoi index for all filtered events
	 in interested tests list_of_test_id, save the voronoi results in voronoi_index_results.json
	 
	 by invoking calculator.voronoi_structural_analysis.run_all_tests_voronoi_calculator
	 with atom_list ='local' when --local is invoked.
	 
	 if --return_volume is invoked, it will also save the voronoi cell volumes into voronoi_volume_results.json
	 in each event directory
	 
	 Note: voronoi index calculation is done by using pyvoro package to provide 
	 a python interface to voro++ C++ library developed by Chris Rycroft at Harvard
	 "Voro++: A three-dimensional Voronoi cell library in C++"
	 
	 > voronoi index classification and visualization:
	 
	 art_data -s SETTINGS --voro --classify will classify voronoi index of previous calculations on selected atoms
	 and plot the results in voronoi_index_results.json for all filtered events 
	 in interested tests list_of_test_id, 
	 and calculate and plot the dynamic transition correlation matrix
	 by using function voronoi_structural_analysis.run_all_tests_voronoi_classifier
	 
	 Note: classification of voronoi indexes can be referred to 
	 Prof En Ma Nature Material paper "Tuning order in disorder" and 
	 "Nanometer-scale gradient atomic packing structure surrounding 
	 soft spots in metallic glasses"
	 
	 > statistics of local atoms for all filtered events in list_of_test_id
	 art_data -s SETTINGS --events_local_atoms will
	 calculate and plot the statistics of number of local atoms and slope for all filtered events 
	 in interested tests list_of_test_id
	 
	 art_data -s SETTINGS --central_atom_max_disp will
	 check if the triggered central atom is the max displacement atom in all 
	 filtered events in list_of_test_id
	 
	 > save data in a csv file for correlation analysis:
	 Before using this function, if users want to act on atom_list == 'initial', they
	 need to ensure that voronoi results are also calculated based on atom_list is "local".
	 If not sure about this, run art_data -s SETTINGS --initial --voro --calc --return_volume --re_calc
	 to redo and override the existing calculations.
	 
	 art_data -s SETTINGS --initial --create_data_table
	 will create a data table with atom_list being initial triggered cluster of atoms
	 and output relevant physical quantities. For physical quantities that are atom-by-atom based,
	 the saved physical quantities act on this group of atom_list by averaging, normalizing, difference of summation
	 
	 User can extract this data csv file from each sample and perform correlation analysis on multiple samples as needed.
	"""
def get_central_atom_list(path_to_data_dir, path_to_input_files, sample_name, sample_type, n_tests):
	path_to_interested_atom_list = os.path.join(path_to_data_dir,"interested_atom_list.json")
	path_to_central_atom_list = os.path.join(path_to_data_dir,"central_atom_list.json")
	# get the path string of dump or lammps data file
	path_to_sample = os.path.join(path_to_input_files, sample_name)
	
	if os.path.isfile(path_to_interested_atom_list):
		try:
			interested_atom_list = json.load(open(path_to_interested_atom_list, 'r'))
		except ValueError:
			raise Exception("%s is an empty file or can not be read"%path_to_interested_atom_list)
	else:
		print "interested_atom_list.json file does not exist, set each atom in the configuration \
		to be central atom"
		
		if sample_type == 'dump':
			config_results = read_data_from_dump(path_to_sample)
		elif sample_type == 'lammps_data':
			config_results = read_data_from_lammps_data(path_to_sample)
		interested_atom_list = config_results['item'].tolist()
		
		with open(path_to_interested_atom_list, 'w+') as f:
			json.dump(interested_atom_list,f)
	
	if len(interested_atom_list) >= n_tests:
		random.seed(0)
		central_atom_list = random.sample(interested_atom_list, n_tests)
	else:
		central_atom_list = interested_atom_list
	
	with open(path_to_central_atom_list, 'w+') as f:
		json.dump(central_atom_list,f)
	return central_atom_list

def get_more_central_atoms(path_to_data_dir, n_tests):
	path_to_interested_atom_list = os.path.join(path_to_data_dir,"interested_atom_list.json")
	path_to_central_atom_list = os.path.join(path_to_data_dir,"central_atom_list.json")
	try:
		interested_atom_list = json.load(open(path_to_interested_atom_list, 'r'))
		central_atom_list = json.load(open(path_to_central_atom_list, 'r'))
	except IOError:
		raise Exception("either interested_atom_list.json or central_atom_list.json file does not exist or is empty!")
	
	left_central_atom_list = [x for x in interested_atom_list if x not in central_atom_list]
	
	if len(left_central_atom_list) >= n_tests:
		random.seed(0)
		new_central_atom_list = random.sample(left_central_atom_list, n_tests)
		final_central_atom_list = central_atom_list + new_central_atom_list
	else:
		new_central_atom_list = left_central_atom_list
		final_central_atom_list = interested_atom_list
	print ">>> updating the central_atom_list.json!!! It will contain both the added new_central_atom_list and previous existing central_atom_list!"
	with open(path_to_central_atom_list, 'w+') as f:
		json.dump(final_central_atom_list,f)

	return new_central_atom_list

def update_input(input_param):
	print "\n >>> updating central_atom_list and list_of_test_id key in input SETTINGS file to be those saved in central_atom_list.json \n"
	path_to_central_atom_list = os.path.join(input_param["path_to_data_dir"],"central_atom_list.json")
	input_param["central_atom_list"] = json.load(open(path_to_central_atom_list, 'r'))
	input_param["list_of_test_id"] = input_param["central_atom_list"]
	return input_param
	
def example_input(n_tests):
	"""
	this function returns an example input file json/dict
	"""
	input = dict()
	input["path_to_data_dir"] = os.environ['DATA_DIR']
	input["path_to_input_files"] = os.environ['ART_INPUT']
	input['sample_name'] = os.environ["ART_SAMPLE"]
	input['sample_type'] = os.environ["SAMPLE_TYPE"]
	
	with nostdout():
		if not os.path.isdir(os.environ['DATA_DIR']):
			print "creating directory: %s"%input["path_to_data_dir"]
			os.makedirs(input["path_to_data_dir"])
		input["central_atom_list"] = get_central_atom_list(input["path_to_data_dir"],input["path_to_input_files"],input['sample_name'],input['sample_type'],n_tests)
		l_range = read_box_range_from_file(input["path_to_input_files"], input['sample_name'], input['sample_type'])
	# create all tests for each atom in central_atoms_list
	# select part tests for further post-processing in list_of_test_id
	# default using all tests, which are created by central_atom_list
	# usually user are interested in a single test post-processing results
	# then manually edit the input file
	input["list_of_test_id"] = input["central_atom_list"]	
	input['box_dim'] = [l_range[0][1] - l_range[0][0], l_range[1][1] - l_range[1][0], l_range[2][1] - l_range[2][0]]
	input['total_energy'] = 0.0
	# for significance level of student t test, check --settings-format
	input['significance'] = 0.05
	
	input['cut_off'] = {str((1,1)):3.7,str((1,2)):3.7,str((2,2)):3.7}
	input['identical_event_criteria'] = {"D_init_fin": 0.1, "E_init_fin": 0.005, "E_init_sad": 0.01}
	input['num_of_proc'] = mp.cpu_count()
	input['re_calc'] = False
	input['atom_list'] = None
	
	# for the outlier detection
	input['model'] = "LinearSVR"
	input['feature'] = "displacement"
	input['target'] =  "shear_strain"
	input['residual_threshold'] = np.arange(0.01, 1.0, 0.01).tolist()
	input['critical_local_atoms_slope'] = -15
	#input['final_residual_threshold'] = 0.54
	
	# for voronoi analysis
	input['voro_cut_off'] = 3.7
	#l_range = [0.299875, 32.130125]
	input['box_range'] = l_range
	input['periodic'] = [True, True, True]
	return input
	
	
if __name__ == "__main__":

  parser = argparse.ArgumentParser(description = 'begin art data processing')
  
  # add command line arguments, order does not matter
  parser.add_argument('--desc', help='Print extended usage description', action="store_true")
  parser.add_argument('--settings-format', help='Print input file description', action="store_true")
  # when --example argument is missing in command line, the default created by store_true is False
  parser.add_argument('--example', nargs=1, help='Print example input file, need one input integer arguments',type=int, metavar=('num of each central atom tests to be created'))
  parser.add_argument('--archive', help='archive an art data project that has already been filtered', action="store_true")
  
  
  parser.add_argument('-s', '--settings', nargs=1, help='Settings input filename, where SETTINGS is the input file name', type=str)
  parser.add_argument('--update_input', help='update input SETTINGS file after run more ART_data', action="store_true")
  parser.add_argument('--delete_tests_files', nargs='+', help='delete multiple files specified in command line for all ART tests directory', type=str)
  
  parser.add_argument('--filter', help='filter events to remove unsuccessful and redudant event', action="store_true")
  parser.add_argument('--find_interested_events', help='find all events satisfying a criteria, current default is to find all events whose act_eng and relax_eng is equal up to its precision', action = "store_true")
  parser.add_argument('--subset', help='use with --max_disp_eng after finding the interested subset of all filtered events --find_interested_events', action = "store_true")
  
  parser.add_argument('--eng', help='perform operations on activation and relaxation energy', action="store_true")
  parser.add_argument('--strain', help='perform operations on atomic strain and displacement', action="store_true")
  parser.add_argument('--voro', help='perform operations on voronoi cell analysis', action="store_true")
  parser.add_argument('--return_volume', help='combine with --voro --calc to output the voronoi cell volumes', action="store_true")
  
  parser.add_argument('-c','--calc', help='perform calculations', action="store_true")
  parser.add_argument('-v', help='plot results', action="store_true")
  parser.add_argument('--ttest_kfold', nargs=3, metavar=('OPTION', 'k', 'n'), help='perform t test on random k folds by n times on a single data')
  parser.add_argument('--stats', help='plot results statistics', action="store_true")
  parser.add_argument('--ttest', nargs=3, metavar=('OPTION', 'PATH_1', 'PATH_2'), help='perform t test to check convergence, need three arguments OPTION, PATH_1, PATH_2, OPTION is either ind or rel, PATH_1 is the path string to data dir 1, PATH_2 is the path string to data dir 2')
  parser.add_argument('--classify', help='classify the results', action="store_true")
  parser.add_argument('--create_data_table', help='create three csv event data files for initial_saddle, saddle_final, initial_final', action="store_true")
  
  
  parser.add_argument('--find_residual', help='find the relative residual threshold for outlier detection in atomic displacement vs strain for all events in specified tests', action = "store_true")
  parser.add_argument('--find_local_index', nargs=1,metavar=('rel_residual_threshold'), type=float, help='find the local atom index in all events in specified tests, where rel_residual_threshold is the user specified relative threshold residual')
  parser.add_argument('--find_triggered_cluster_atoms_index', help='find the initial triggered cluster atoms indexes for all filtered events in list_of_test_id tests, saving cluster atom indexes under each event directory', action = "store_true")
  parser.add_argument('--find_central_index', help='find the central atom index for all filtered events in list_of_test_id tests, saving central atom index under each event directory', action = "store_true")
  parser.add_argument('--find_max_disp_index', help='find the max displaced atom index during the initial to saddle process for all filtered events in list_of_test_id tests, saving max_disp atom index under each event directory',action = "store_true")
  parser.add_argument('--find_pn_index', help='find the number and indexes of involved atoms by the physical definition of participation number for all filtered events in list_of_test_id tests',action = "store_true")
  parser.add_argument('--find_pn_non_affine_index', help='find the number and indexes of involved atoms by the physical definition of participation number using atomic shear strain for all filtered events in list_of_test_id tests',action = "store_true")
  
  parser.add_argument('--local', help='invoke calculations on local atoms that are determined by --find_local_index', action="store_true")
  parser.add_argument('--initial', help='invoke calculations on initial triggered cluster atoms that are determined by --find_triggered_cluster_atoms_index', action="store_true")
  parser.add_argument('--central', help='invoke calculations on central atom that are determined by --find_central_index', action="store_true")
  parser.add_argument('--max_disp', help='invoke calculations on max displaced atoms during init to sad that are determined by --find_max_disp_index', action="store_true")
  parser.add_argument('--pn', help='invoke calculations on pn atoms that are determined by --find_pn_index', action="store_true")
  parser.add_argument('--pn_non_affine', help='invoke calculations on pn atoms that are determined by --find_pn_non_affine_index', action="store_true")
  
  parser.add_argument('--cluster_shear_vol', help='combine with --strain --local to plot the local event cluster averaged shear strain vs cluster averaged volumetric strain for all filtered events in list_of_test_id', action="store_true")
  parser.add_argument('--atom_shear_vol', help='combine with --strain --local to plot the shear strain vs volumetric strain of all local atoms for all filtered events in list_of_test_id', action="store_true")
  parser.add_argument('--events_local_atoms',nargs=1,metavar=('rel_residual_threshold'), type=float, help='calculate the average local atoms and their statistics in all filtered events in specified tests at a user specified residual threshold')
  parser.add_argument('--central_atom_max_disp', help='check if the central atom is the maximum displacement atom for all filtered events in specified tests', action = "store_true")
  parser.add_argument('--max_disp_eng', help='plot max displacement vs activation energy and relaxation energy', action = "store_true")
  parser.add_argument('--convert_to_ovito', help='convert a single event configuration files from refconfig type into lammps data type for Ovito visualization', nargs=4, metavar=('test_id', 'init', 'sad','fin'))
  
  #parser.add_argument('--strain_calc', help='perform atomic strain and displacement calculations', action="store_true")
  #parser.add_argument('--strain_v', help='perform atomic strain and displacement calculations', action="store_true")
  
  parser.add_argument('-p', '--path', nargs=1, help='Set up path to data directory, overrides the path in user specified input file', type=str)
  parser.add_argument('-np', nargs=1, help='set up number of processors to run the jobs, default 1 without specifying, will be overided by user specified input file', type=int, default=1)
  parser.add_argument('--re_calc', help='redo the calculations by setting the re_calc argument, will override the re_calc argument in user specified input file', action="store_true")
  
  
  # check the default attribute name of dest
  parser.add_argument('-q','--quiet', help='Quiet output', action="store_true", default=False)
  
  # to be implemented in the future
  # integration with running art
  parser.add_argument('--art', help='setting up ART input files, and running art in parallel with np specified in SETTINGS file', action='store_true')
  parser.add_argument('--slurm', nargs=1, help='number of compute nodes to use for running simulations by submitting jobs in slurm cluster', type=int)
  
  # integration with lammps
  parser.add_argument('--lammps',help='begin lammps mode for generating initial ART samples, to be implemented',action="store_true")
  
  parser.add_argument('--run', help='begin running simulations after --lammps or --art mode', action="store_true")
  
  parser.add_argument('--run_more', nargs=1, metavar=('NUM_OF_TESTS'), help='running more ART simulations if ART data is not converged', type=int)
  parser.add_argument('--sync_central_atoms',help='sync the central_atom_list.json to be the subdir name in path_to_data_dir',action="store_true")
  parser.add_argument('--delete_tests', help='combine with --art to delete all test directories whose test id stored in central_atom_list of input SETTINGS file', action="store_true")
  parser.add_argument('--delete_unused_events', help='combine with --art to delete all unused events configuration files that are not saved in final_selected_events.json', action="store_true")
  parser.add_argument('--check_tests_status', help='combine with --art to check the status of all tests whose test id stored in central_atom_list of input SETTINGs file', action="store_true")
  
  #parser.add_argument('-v','--verbose', help='verbose output', action="store_true", default=False)
  
  
  args = parser.parse_args()
  
  args.verbose = not args.quiet
  
  if args.settings_format:
	print_input_help()
	exit()
  
  if args.example:
	#print jsonpickle.encode(example_input())
	print json.dumps(example_input(args.example[0]), indent=2)
	#print example_input()
	exit()
  if args.path:
	path_to_data_dir = args.path[0]
  
  if args.lammps and args.art:
	  raise Exception("can only run one simulation at a time")
  if args.lammps:
	  if args.run:
		  run_lammps(args.run[0])
		  exit()  
	  	  
  if args.settings:
	if args.verbose:
		print "Loading", args.settings[0]
	#input_param = jsonpickle.decode(open(args.settings[0],'r').read())
	#input_param = json.loads(open(args.settings[0], 'r').read())
	input_param = json.load(open(args.settings[0], 'r'))
	
	if args.update_input:
		updated_input_param = update_input(input_param)
		json.dump(updated_input_param, open(args.settings[0], 'w'), indent=2)
		exit()
	
	new_cut_off_dict = dict()
	for k,v in input_param['cut_off'].items():
		new_cut_off_dict[eval(k)] = v
	input_param['cut_off'] = new_cut_off_dict
	
	if 'num_of_proc' not in input_param:
		if args.np <= mp.cpu_count():
			input_param['num_of_proc'] = args.np
		else:
			raise Exception("number of cores specified by -np must be no greater than %s"%mp.cpu_count())
	if (not args.path) and ("path_to_data_dir" in input_param):
		path_to_data_dir = input_param["path_to_data_dir"]
	
	if "re_calc" not in input_param:
		input_param["re_calc"] = False
	if args.re_calc:
		input_param["re_calc"] = True
	if args.sync_central_atoms:
		sync_central_atom_list_w_curr_subdir(path_to_data_dir)
		exit()
	if args.art:
		if args.run:
		  print "being setting up the ART input files!"
		  set_up_input_files(path_to_data_dir, input_param)
		  print "starting running art in parallel"
		  if args.slurm:
			  input_param['num_of_proc'] = args.slurm
			  run_art_cluster_slurm(path_to_data_dir, input_param)
		  else:
			  run_art_mp(path_to_data_dir, input_param)
		  #run_art_mp(args.run[0])
		  exit()
		if args.run_more:
		  print ">>>ART_data is not converged, running more ART data!"
		  # it will update the central_atom_list.json with both added new_central_atom_list and old_central_atom_list
		  # it only return new_central_atom_list for artn calculations
		  new_central_atom_list = get_more_central_atoms(path_to_data_dir, args.run_more[0])
		  print ">>>get new central atom list"
		  input_param["central_atom_list"] = new_central_atom_list
		  print ">>>setting up the ART input files for new central atom list!"
		  set_up_input_files(path_to_data_dir, input_param)
		  print ">>>starting running art for new central_atom_list in parallel!"
		  if args.slurm:
			  input_param['num_of_proc'] = args.slurm
			  run_art_cluster_slurm(path_to_data_dir, input_param)
		  else:
			  run_art_mp(path_to_data_dir, input_param)
		  #run_art_mp(args.run[0])
		  
		  input_param = update_input(input_param)
		  		  
		  new_cut_off_dict = dict()
		  for k,v in input_param['cut_off'].items():
			new_cut_off_dict[str(k)] = v
		  input_param['cut_off'] = new_cut_off_dict
		  
		  json.dump(input_param, open(args.settings[0], 'w'), indent=2)
		  exit()
		if args.delete_tests_files:
		  file_names = args.delete_tests_files
		  print "deleting files :", file_names, "in %s"%path_to_data_dir
		  delete_art_tests_files(path_to_data_dir, file_names)
		  exit()
		if args.delete_tests:
		  central_atom_list = input_param["central_atom_list"]
		  print "deleting tests:",central_atom_list, "in %s"%path_to_data_dir
		  delete_art_tests(path_to_data_dir,central_atom_list)
		  exit()
		if args.delete_unused_events:
		  delete_unused_events_data(path_to_data_dir,input_param)
		  exit()
		if args.check_tests_status:
		  print "begin checking status of tests in central_atom_list of current input SETTINGS file"
		  new_cut_off_dict = dict()
		  for k,v in input_param['cut_off'].items():
			new_cut_off_dict[str(k)] = v
		  input_param['cut_off'] = new_cut_off_dict
		  check_tests_status(path_to_data_dir, input_param)
		  exit()

	if args.filter:
		#if args.path:
		#	path_to_data_dir = args.path[0]
		print "current data dir:", path_to_data_dir
		print "begining filter events in stage I two criteria"
		filter_events_all_tests_stage_1(path_to_data_dir, input_param)
		print "begining filter events in stage II 1 criteria"
		filter_events_all_tests_stage_2(path_to_data_dir, input_param)
		exit()
	
	if args.eng:
		print "begin operation on activation and relaxation energy,--path argument overides path_to_data_dir in input file"
		#if args.path:
		#	path_to_data_dir = args.path[0]
		if args.calc:
			print "beging calculating activation and relaxation energy distribution for all filtered events in input file list_of_test_id of %s"%path_to_data_dir
			energy_calculator_run_all_tests_mp(path_to_data_dir, input_param)
			exit()
		if args.ttest_kfold:
			print "being performing t test"
			print "t test mode is", args.ttest_kfold[0]
			print "random %s fold number"%args.ttest_kfold[1]
			print "split data into k fold into %s times"%args.ttest_kfold[2]
			try:
				is_converged = eng_k_fold_ttest(path_to_data_dir, input_param['significance'], int(args.ttest_kfold[1]), args.ttest_kfold[0], int(args.ttest_kfold[2]))
			except KeyError:
				is_converged = eng_k_fold_ttest(path_to_data_dir, k=int(args.ttest_kfold[1]), option=args.ttest_kfold[0], n=int(args.ttest_kfold[2]))

			if is_converged == True:
				print "activation and relaxation energy converged"
			else:
				print "activation and relaxation energy did NOT converged!"
			exit()
			
		if args.ttest:
			print "being performing t test"
			print "t test mode is", args.ttest[0]
			print "path to ART data dir 1:", args.ttest[1]
			print "path to ART data dir 2:", args.ttest[2]
			if args.ttest[0] == 'ind':
				# ttest_ind has equal_var = False option, default True
				try:
					is_converged = eng_convergence_ttest_ind(args.ttest[1], args.ttest[2],input_param['significance'])
				except KeyError:
					is_converged = eng_convergence_ttest_ind(args.ttest[1], args.ttest[2])
			elif args.ttest[0] == 'rel':
				try:
					is_converged = eng_convergence_ttest_rel(args.ttest[1], args.ttest[2],input_param['significance'])
				except KeyError:
					is_converged = eng_convergence_ttest_rel(args.ttest[1], args.ttest[2])
			if is_converged == True:
				print "activation and relaxation energy converged"
			else:
				print "activation and relaxation energy did NOT converged!"
			exit()
	
	if args.strain:
		if sum([args.local, args.initial, args.central, args.max_disp, args.pn, args.pn_non_affine])>=2:
			raise Exception("strain calculations can only act on 1 set of atoms, use only one of --local, --initial, --central, --max_disp, --pn, --pn_non_affine")
		if args.local:
			input_param["atom_list"] = "local"
		elif args.initial:
			input_param["atom_list"] = "initial"
		elif args.central:
			input_param["atom_list"] = "central"
		elif args.max_disp:
			input_param["atom_list"] = "max_disp"
		elif args.pn:
			input_param["atom_list"] = "pn"
		elif args.pn_non_affine:
			input_param["atom_list"] = "pn_non_affine"
		
		if args.cluster_shear_vol:
			shear_strain_vol_strain_cluster_all_events(path_to_data_dir, input_param)
			exit()
		if args.atom_shear_vol:
			shear_strain_vol_strain_local_atom_all_events(path_to_data_dir, input_param)
			exit()
		if (args.calc and args.v) or (args.calc and args.stats) or (args.stats and args.v):
			raise Exception("operations on strain can not be more than 1 at one time")
		if args.calc:
			# strain calculation is invoked
			start_time = time.time()
			strain_calculator_run_all_tests_mp(path_to_data_dir, input_param)
			print "total run time:", time.time() - start_time, "seconds"
			exit()
		elif args.v:
			events_strain_visualization(path_to_data_dir, input_param)
			exit()
		elif args.stats:
			strain_events_stats_visualization(path_to_data_dir, input_param)
			exit()
	
	if args.max_disp_eng:
		if args.subset:
			eng_max_disp(path_to_data_dir, input_param, subset=True)
			exit()
		eng_max_disp(path_to_data_dir, input_param)
		exit()
	if args.convert_to_ovito:
		event_state = [args.convert_to_ovito[0], [args.convert_to_ovito[1],args.convert_to_ovito[2],args.convert_to_ovito[3]]]
		refconfig_to_lammps_data(path_to_data_dir, event_state, input_param)
		exit()
	if args.find_residual:
		print "start finding relative residual threshold for outlier detection"
		residual_threshold_finder(path_to_data_dir, input_param)
		exit()
	if args.find_interested_events:
		find_act_relax_equal_events(path_to_data_dir, input_param)
		exit()
	if args.find_local_index:
		# if --find_local_index was set a default =0.54, then args.find_local_index is a float, not a list
		residual_threshold = args.find_local_index[0]
		all_events_local_atoms_finder(path_to_data_dir, input_param, residual_threshold)
		exit()
	if args.find_triggered_cluster_atoms_index:
		all_events_triggered_cluster_atoms_finder(path_to_data_dir, input_param)
		exit()
	
	if args.find_central_index:
		all_events_central_atom_finder(path_to_data_dir, input_param)
		exit()
	if args.find_max_disp_index:
		all_events_max_disp_atom_finder(path_to_data_dir, input_param)
		exit()
	if args.find_pn_index:
		pn_calculator_run_all_tests_mp(path_to_data_dir, input_param)
		exit()
	if args.find_pn_non_affine_index:
		pn_calculator_run_all_tests_mp(path_to_data_dir, input_param, is_non_affine = True)
		exit()
	if args.events_local_atoms:
		residual_threshold = args.events_local_atoms[0]
		events_local_atoms(path_to_data_dir, input_param, residual_threshold)
		exit()
		
	if args.central_atom_max_disp:
		run_tests_triggered_atom_is_max_disp(path_to_data_dir, input_param)
		exit()
	if args.archive:
		archive_project(path_to_data_dir)
		exit()
	if args.voro:
		if sum([args.local, args.initial, args.central, args.max_disp, args.pn, args.pn_non_affine])>=2:
			raise Exception("calculations can only act on 1 set of atoms, use only one of --local, --initial, --central, --max_disp, --pn, --pn_non_affine")
		if args.local:
			input_param["atom_list"] = "local"
		elif args.initial:
			input_param["atom_list"] = "initial"
		elif args.central:
			input_param["atom_list"] = "central"
		elif args.max_disp:
			input_param["atom_list"] = "max_disp"
		elif args.pn:
			input_param["atom_list"] = "pn"
		elif args.pn_non_affine:
			input_param["atom_list"] = "pn_non_affine"

		if args.calc:
			input_param['cut_off'] = input_param['voro_cut_off']
			start_time = time.time()
			run_all_tests_voronoi_calculator(path_to_data_dir,input_param,return_volume = args.return_volume)
			print "total run time:", time.time() - start_time, "seconds"
			exit()
		elif args.classify:
			start_time = time.time()
			run_all_tests_voronoi_classifier(path_to_data_dir,input_param)
			print "total run time:", time.time() - start_time, "seconds"
			exit()
	if args.create_data_table:
		if sum([args.local, args.initial, args.central, args.max_disp, args.pn, args.pn_non_affine])>=2:
			raise Exception("calculations can only act on 1 set of atoms, use only one of --local, --initial, --central, --max_disp, --pn, --pn_non_affine")
		if args.local:
			input_param["atom_list"] = "local"
		elif args.initial:
			input_param["atom_list"] = "initial"
		elif args.central:
			input_param["atom_list"] = "central"
		elif args.max_disp:
			input_param["atom_list"] = "max_disp"
		elif args.pn:
			input_param["atom_list"] = "pn"
		elif args.pn_non_affine:
			input_param["atom_list"] = "pn_non_affine"
		
		generate_correlation_table_mp(path_to_data_dir, input_param)
		exit()
  elif args.desc:
	print_desc_help()
  else:
	parser.print_help()
